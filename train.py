import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from data_preprocessing import TextPreprocessor, FeatureExtractor
from models import TraditionalModels, LSTMModel, ModelEvaluator
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
import sys
import warnings

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

warnings.filterwarnings('ignore')


class SentimentAnalysisTrainer:
    def __init__(self, language='chinese', data_dir='./'):
        self.language = language
        self.data_dir = data_dir
        self.preprocessor = TextPreprocessor(language)
        self.feature_extractor = FeatureExtractor(language=language)
        self.traditional_models = TraditionalModels()
        self.lstm_model = None
        self.evaluator = ModelEvaluator()

    def load_dataset_from_csv(self, csv_filename, text_column='review', label_column='label'):
        """
        ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®é›†ï¼Œè‡ªåŠ¨é€‚é…ä¸­æ–‡é…’åº—è¯„è®ºå’ŒIMDbç”µå½±è¯„è®ºæ ¼å¼ã€‚
        è¿”å›ï¼šåŸå§‹æ–‡æœ¬åˆ—è¡¨ï¼Œæ ‡ç­¾åˆ—è¡¨
        """
        csv_path = os.path.join(self.data_dir, csv_filename)

        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°: {csv_path}")

        print(f"æ­£åœ¨ä» {csv_filename} åŠ è½½æ•°æ®é›†...")
        df = pd.read_csv(csv_path)
        print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")
        print(f"æ•°æ®åˆ—å: {list(df.columns)}")

        # è‡ªåŠ¨æ£€æµ‹åˆ—å
        if text_column not in df.columns:
            # å°è¯•å¸¸è§çš„ä¸­æ–‡åˆ—å
            chinese_text_cols = ['review', 'text', 'comment', 'è¯„è®º', 'å†…å®¹']
            for col in chinese_text_cols:
                if col in df.columns:
                    text_column = col
                    print(f"è‡ªåŠ¨æ£€æµ‹åˆ°æ–‡æœ¬åˆ—: {text_column}")
                    break

        if label_column not in df.columns:
            # å°è¯•å¸¸è§çš„æ ‡ç­¾åˆ—å
            label_cols = ['label', 'sentiment', 'emotion', 'è¯„åˆ†', 'æƒ…æ„Ÿ']
            for col in label_cols:
                if col in df.columns:
                    label_column = col
                    print(f"è‡ªåŠ¨æ£€æµ‹åˆ°æ ‡ç­¾åˆ—: {label_column}")
                    break

        texts = df[text_column].astype(str).fillna('').tolist()
        labels = df[label_column].tolist()

        # æ ‡ç­¾æ ¼å¼ç»Ÿä¸€åŒ–
        if isinstance(labels[0], str):
            print(f"æ­£åœ¨è½¬æ¢æ ‡ç­¾æ ¼å¼ï¼ˆæ£€æµ‹åˆ°å­—ç¬¦ä¸²æ ‡ç­¾ï¼‰...")
            label_mapping = {'positive': 1, 'negative': 0, 'æ­£é¢': 1, 'è´Ÿé¢': 0, '1': 1, '0': 0}
            labels = [label_mapping.get(str(label).lower().strip(), 0) for label in labels]

        # ç¡®ä¿æ ‡ç­¾æ˜¯æ•´æ•°
        labels = [int(label) for label in labels]

        # æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§
        pos_count = sum(labels)
        neg_count = len(labels) - pos_count
        print(f"æ­£é¢æ ·æœ¬: {pos_count} æ¡ï¼Œè´Ÿé¢æ ·æœ¬: {neg_count} æ¡")
        print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(texts)} æ¡è®°å½•")

        return texts, labels

    def load_sample_data(self):
        """
        æ ¹æ®è¯­è¨€åŠ è½½ç›¸åº”çš„æ•°æ®é›†
        """
        if self.language == 'chinese':
            # ä¸­æ–‡é…’åº—è¯„è®ºæ•°æ®é›†
            try:
                texts, labels = self.load_dataset_from_csv(
                    'ChnSentiCorp_htl_all.csv',
                    text_column='review',
                    label_column='label'
                )
                return texts, labels
            except FileNotFoundError:
                print("è­¦å‘Šï¼šæœªæ‰¾åˆ°ä¸­æ–‡æ•°æ®é›†æ–‡ä»¶ï¼Œä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®")
                return self._get_sample_chinese_data()
        else:
            # è‹±æ–‡IMDbç”µå½±è¯„è®ºæ•°æ®é›†
            try:
                texts, labels = self.load_dataset_from_csv(
                    'IMDB Dataset.csv',
                    text_column='review',
                    label_column='sentiment'
                )
                return texts, labels
            except FileNotFoundError:
                print("è­¦å‘Šï¼šæœªæ‰¾åˆ°è‹±æ–‡æ•°æ®é›†æ–‡ä»¶ï¼Œä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®")
                return self._get_sample_english_data()

    def _get_sample_chinese_data(self):
        """ä¸­æ–‡ç¤ºä¾‹æ•°æ®"""
        texts = [
            "è¿™ä¸ªäº§å“éå¸¸å¥½ç”¨ï¼Œè´¨é‡å¾ˆæ£’ï¼",
            "éå¸¸å¤±æœ›ï¼Œäº§å“è´¨é‡å¾ˆå·®",
            "æ€§ä»·æ¯”é«˜ï¼Œæ¨èè´­ä¹°",
            "å®Œå…¨ä¸å€¼å¾—è¿™ä¸ªä»·æ ¼",
            "æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œç‰©æµå¾ˆå¿«",
            "åŒ…è£…ç ´æŸï¼Œä½“éªŒå¾ˆå·®",
            "ç‰©è¶…æ‰€å€¼ï¼Œéå¸¸æ»¡æ„",
            "è´¨é‡ä¸€èˆ¬ï¼Œæ²¡æœ‰æƒ³è±¡ä¸­å¥½",
        ]
        labels = [1, 0, 1, 0, 1, 0, 1, 0]
        return texts, labels

    def _get_sample_english_data(self):
        """è‹±æ–‡ç¤ºä¾‹æ•°æ®"""
        texts = [
            "This movie is fantastic, great acting!",
            "Terrible movie, waste of time",
            "Amazing plot and characters",
            "Boring and poorly made",
            "One of the best films I've seen",
            "Disappointing and overrated",
            "Absolutely loved it, highly recommend",
            "Not as good as expected",
        ]
        labels = [1, 0, 1, 0, 1, 0, 1, 0]
        return texts, labels

    def train_traditional_models(self, texts, labels, feature_method='tfidf', test_size=0.2):
        """
        è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆæœ´ç´ è´å¶æ–¯ã€SVMã€é›†æˆæ¨¡å‹ï¼‰
        ä¿®å¤ï¼šç¡®ä¿è®­ç»ƒ/æµ‹è¯•é›†åœ¨é¢„å¤„ç†å‰åˆ†å‰²
        """
        # 1. å…ˆåˆ†å‰²æ•°æ®é›†
        train_texts, test_texts, train_labels, test_labels = train_test_split(
            texts, labels, test_size=test_size, random_state=42, stratify=labels
        )

        print(f"ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒé›†: {len(train_texts)} æ¡ï¼Œæµ‹è¯•é›†: {len(test_texts)} æ¡")

        # 2. åˆ†åˆ«é¢„å¤„ç†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        print("é¢„å¤„ç†è®­ç»ƒé›†æ–‡æœ¬...")
        processed_train_texts, train_labels = self.preprocessor.preprocess_dataset(train_texts, train_labels)

        print("é¢„å¤„ç†æµ‹è¯•é›†æ–‡æœ¬...")
        processed_test_texts, test_labels = self.preprocessor.preprocess_dataset(test_texts, test_labels)

        if len(processed_train_texts) == 0 or len(processed_test_texts) == 0:
            print("é”™è¯¯ï¼šé¢„å¤„ç†åæ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬")
            return {}, None, None, None, None

        # 3. ç‰¹å¾æå–ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼‰
        print("ç‰¹å¾æå–ï¼ˆTF-IDFï¼‰...")
        self.feature_extractor.fit_tfidf(processed_train_texts, max_features=5000)

        X_train = self.feature_extractor.transform_tfidf(processed_train_texts)
        X_test = self.feature_extractor.transform_tfidf(processed_test_texts)

        # 4. è®­ç»ƒæ¨¡å‹
        print("è®­ç»ƒæœ´ç´ è´å¶æ–¯æ¨¡å‹...")
        nb_model = self.traditional_models.train_naive_bayes(X_train, train_labels)

        print("è®­ç»ƒSVMæ¨¡å‹...")
        svm_model = self.traditional_models.train_svm(X_train, train_labels, kernel='linear')

        print("è®­ç»ƒé›†æˆæ¨¡å‹...")
        ensemble_model = self.traditional_models.create_ensemble(X_train, train_labels)

        # 5. è¯„ä¼°æ¨¡å‹
        models_to_evaluate = ['naive_bayes', 'svm', 'ensemble']
        results = {}

        for model_name in models_to_evaluate:
            y_pred = self.traditional_models.predict(model_name, X_test)
            y_proba = self.traditional_models.predict_proba(model_name, X_test)
            metrics = self.evaluator.evaluate_model(test_labels, y_pred, y_proba)
            self.evaluator.print_metrics(metrics, model_name)
            results[model_name] = metrics

        return results, X_train, X_test, train_labels, test_labels

    def train_lstm_model(self, texts, labels, test_size=0.2, vocab_size=5000, max_length=100):
        """
        è®­ç»ƒLSTMæ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬
        æ ¸å¿ƒä¿®å¤ï¼šå…ˆåˆ†å‰²æ•°æ®é›†ï¼Œå†åˆ†åˆ«å‡†å¤‡åºåˆ—æ•°æ®ï¼Œé¿å…æ•°æ®æ³„éœ²
        """
        # 1. å…ˆåˆ†å‰²åŸå§‹æ•°æ®é›†
        train_texts, test_texts, train_labels, test_labels = train_test_split(
            texts, labels, test_size=test_size, random_state=42, stratify=labels
        )

        print(f"LSTMè®­ç»ƒé›†: {len(train_texts)} æ¡ï¼Œæµ‹è¯•é›†: {len(test_texts)} æ¡")

        # 2. åˆ†åˆ«é¢„å¤„ç†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        print("é¢„å¤„ç†è®­ç»ƒé›†æ–‡æœ¬...")
        processed_train_texts, train_labels = self.preprocessor.preprocess_dataset(train_texts, train_labels)

        print("é¢„å¤„ç†æµ‹è¯•é›†æ–‡æœ¬...")
        processed_test_texts, test_labels = self.preprocessor.preprocess_dataset(test_texts, test_labels)

        if len(processed_train_texts) == 0 or len(processed_test_texts) == 0:
            print("é”™è¯¯ï¼šé¢„å¤„ç†åæ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬")
            return {}, None

        # 3. å‡†å¤‡LSTMæ•°æ® - å…³é”®ä¿®å¤ï¼šåªåœ¨è®­ç»ƒé›†ä¸Šæ‹ŸåˆTokenizer
        print("å‡†å¤‡LSTMåºåˆ—æ•°æ®ï¼ˆä¸¥æ ¼éš”ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼‰...")
        self.lstm_model = LSTMModel(vocab_size=vocab_size, max_length=max_length, embedding_dim=100)

        # åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆtokenizer
        self.lstm_model.tokenizer = Tokenizer(num_words=vocab_size)
        self.lstm_model.tokenizer.fit_on_texts(processed_train_texts)

        # åˆ†åˆ«è½¬æ¢è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train = self.lstm_model.prepare_texts(processed_train_texts)
        X_test = self.lstm_model.prepare_texts(processed_test_texts)

        y_train = np.array(train_labels)
        y_test = np.array(test_labels)

        # 4. è¿›ä¸€æ­¥åˆ†å‰²å‡ºéªŒè¯é›†
        X_train_final, X_val, y_train_final, y_val = train_test_split(
            X_train, y_train, test_size=0.1, random_state=42, stratify=y_train
        )

        print(f"LSTMæœ€ç»ˆè®­ç»ƒé›†: {len(X_train_final)} æ¡ï¼ŒéªŒè¯é›†: {len(X_val)} æ¡ï¼Œæµ‹è¯•é›†: {len(X_test)} æ¡")

        # 5. è®­ç»ƒæ¨¡å‹ï¼ˆæ·»åŠ æ—©åœï¼‰
        print("è®­ç»ƒLSTMæ¨¡å‹...")
        history = self.lstm_model.train(
            X_train_final, y_train_final,
            X_val=X_val, y_val=y_val,
            epochs=10,
            batch_size=32
        )

        # 6. è¯„ä¼°æ¨¡å‹
        print("è¯„ä¼°LSTMæ¨¡å‹...")
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„prepare_textsæ–¹æ³•ï¼Œä½†å·²ç»æ˜¯æµ‹è¯•é›†æ•°æ®
        y_pred, y_proba = self.lstm_model.predict(processed_test_texts)
        metrics = self.evaluator.evaluate_model(y_test, y_pred, y_proba)
        self.evaluator.print_metrics(metrics, "LSTM")

        return metrics, history

    def save_models(self, model_dir='./models'):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
            print(f"åˆ›å»ºæ¨¡å‹ç›®å½•: {model_dir}")

        # ä¿å­˜ä¼ ç»Ÿæ¨¡å‹
        traditional_path = os.path.join(model_dir, 'traditional_models.pkl')
        joblib.dump(self.traditional_models, traditional_path)

        # ä¿å­˜ç‰¹å¾æå–å™¨
        feature_path = os.path.join(model_dir, 'feature_extractor.pkl')
        joblib.dump(self.feature_extractor, feature_path)

        # ä¿å­˜LSTMæ¨¡å‹
        if self.lstm_model and self.lstm_model.model:
            lstm_path = os.path.join(model_dir, 'lstm_model.h5')
            self.lstm_model.model.save(lstm_path)
            print(f"LSTMæ¨¡å‹ä¿å­˜åˆ°: {lstm_path}")

        print(f"æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir}")

    def plot_results(self, results, save_path='model_comparison.png'):
        """ç»˜åˆ¶ç»“æœå¯¹æ¯”å›¾"""
        if not results:
            print("æ²¡æœ‰ç»“æœå¯ç»˜åˆ¶")
            return

        models = list(results.keys())
        metrics_names = ['accuracy', 'precision', 'recall', 'f1_score']

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()

        colors = plt.cm.Set3(np.linspace(0, 1, len(models)))

        for i, metric in enumerate(metrics_names):
            values = [results[model][metric] for model in models]

            bars = axes[i].bar(models, values, color=colors, edgecolor='black')
            axes[i].set_title(f'{metric.upper()} å¯¹æ¯”', fontsize=14, fontweight='bold')
            axes[i].set_ylabel(metric.capitalize(), fontsize=12)
            axes[i].set_ylim(0, 1.05)
            axes[i].tick_params(axis='x', rotation=45)

            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
            for bar, v in zip(bars, values):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width() / 2., height + 0.02,
                             f'{v:.4f}', ha='center', va='bottom', fontsize=10)

            # æ·»åŠ ç½‘æ ¼çº¿
            axes[i].grid(True, alpha=0.3, linestyle='--')

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜ä¸º: {save_path}")
        plt.show()


def train_chinese_models():
    """è®­ç»ƒä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹"""
    print("=" * 60)
    print("å¼€å§‹è®­ç»ƒä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹")
    print("=" * 60)

    trainer = SentimentAnalysisTrainer(language='chinese')

    try:
        # 1. åŠ è½½æ•°æ®
        texts, labels = trainer.load_sample_data()

        # 2. è®­ç»ƒä¼ ç»Ÿæ¨¡å‹
        print("\n" + "-" * 40)
        print("è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹")
        print("-" * 40)
        traditional_results, _, _, _, _ = trainer.train_traditional_models(
            texts, labels, test_size=0.2
        )

        # 3. è®­ç»ƒLSTMæ¨¡å‹
        print("\n" + "-" * 40)
        print("è®­ç»ƒLSTMæ·±åº¦å­¦ä¹ æ¨¡å‹")
        print("-" * 40)
        lstm_results, _ = trainer.train_lstm_model(
            texts, labels,
            test_size=0.2,
            vocab_size=5000,
            max_length=100
        )

        # 4. åˆå¹¶ç»“æœå¹¶å¯è§†åŒ–
        all_results = {**traditional_results, 'LSTM': lstm_results}
        trainer.plot_results(all_results, 'chinese_model_comparison.png')

        # 5. ä¿å­˜æ¨¡å‹
        trainer.save_models('chinese_models')

        print("\nâœ… ä¸­æ–‡æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        return True

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def train_english_models():
    """è®­ç»ƒè‹±æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹"""
    print("=" * 60)
    print("å¼€å§‹è®­ç»ƒè‹±æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹")
    print("=" * 60)

    trainer = SentimentAnalysisTrainer(language='english')

    try:
        # 1. åŠ è½½æ•°æ®
        texts, labels = trainer.load_sample_data()

        # å¯¹äºå¤§æ•°æ®é›†ï¼Œå¯ä»¥å…ˆé‡‡æ ·ä¸€éƒ¨åˆ†è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        if len(texts) > 10000:
            print("æ•°æ®é›†è¾ƒå¤§ï¼Œé‡‡æ ·10000æ¡è¿›è¡Œè®­ç»ƒ...")
            # ä¿æŒæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
            from sklearn.utils import resample
            sample_size = min(10000, len(texts))
            texts, labels = resample(texts, labels, n_samples=sample_size, random_state=42, stratify=labels)

        # 2. è®­ç»ƒä¼ ç»Ÿæ¨¡å‹
        print("\n" + "-" * 40)
        print("è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹")
        print("-" * 40)
        traditional_results, _, _, _, _ = trainer.train_traditional_models(
            texts, labels, test_size=0.2
        )

        # 3. è®­ç»ƒLSTMæ¨¡å‹
        print("\n" + "-" * 40)
        print("è®­ç»ƒLSTMæ·±åº¦å­¦ä¹ æ¨¡å‹")
        print("-" * 40)
        lstm_results, _ = trainer.train_lstm_model(
            texts, labels,
            test_size=0.2,
            vocab_size=8000,
            max_length=150
        )

        # 4. åˆå¹¶ç»“æœå¹¶å¯è§†åŒ–
        all_results = {**traditional_results, 'LSTM': lstm_results}
        trainer.plot_results(all_results, 'english_model_comparison.png')

        # 5. ä¿å­˜æ¨¡å‹
        trainer.save_models('english_models')

        print("\nâœ… è‹±æ–‡æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        return True

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("æ–‡æœ¬æƒ…æ„Ÿåˆ†ææ¨¡å‹è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)

    # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“ï¼ˆå¦‚æœéœ€è¦ï¼‰
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

    # è®­ç»ƒä¸­æ–‡æ¨¡å‹
    success_chinese = train_chinese_models()

    if success_chinese:
        print("\n" + "=" * 60)
        choice = input("æ˜¯å¦ç»§ç»­è®­ç»ƒè‹±æ–‡æ¨¡å‹? (y/n): ").strip().lower()

        if choice == 'y' or choice == 'yes':
            # è®­ç»ƒè‹±æ–‡æ¨¡å‹
            success_english = train_english_models()

            if success_english:
                print("\nğŸ‰ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
                print("ä¸­æ–‡æ¨¡å‹ä¿å­˜åœ¨: chinese_models/")
                print("è‹±æ–‡æ¨¡å‹ä¿å­˜åœ¨: english_models/")
            else:
                print("\nâš ï¸ è‹±æ–‡æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œä½†ä¸­æ–‡æ¨¡å‹å·²ä¿å­˜")
        else:
            print("\nâœ… ä¸­æ–‡æ¨¡å‹è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: chinese_models/")
    else:
        print("\nâŒ æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    main()